
# MonkeyDB DATABASE MANAGER

**App creator: Marco Giancola**

*Artificial Intelligence - UniPv - UniMi - UniMiB -*

---

### GENERAL DESCRIPTION
MonkeyDB manager is an application meant to be an academic project, whose
creation helped the creator of this script (Marco Giancola) to learn how
to exploit some standard libraries like PyQt5 to design a basic User
Interface, how to implement a simple hashing system for passwords and,
most importantly, how to develop a computer science project in a quite
rigorous way.

In this project I provided the basic functionalities of a simple database
manager:
- A query portal where it is possible to run SQLite queries
- A results section where the results of the queries are plotted using 
    a table
- A chronology section to keep track of the previous queries
- A tree-structured schema plotting all the .db files in the directory
    the user is working in, and the relative tables of each database

Moreover, I wanted to add some non-trivial functionalities:
- The possibility to export the results of a query in a `.json` file
- An AI model translating natural English in SQL queries

**Important**

As I previously explained, this is an academic project, it is not meant to
be neither a professional database manager nor a commercialized application

---

### DATABASE MANAGMENT SYSTEM
Almost the whole database managment system of the application revolves around
the `sqlite3` python library.

Main features of MonkeyDB's implementation:
- Each database is a .db file stored in the working directory
- The following operations can be performed on databases and tables from the graphical interface: **delete**, **create** (only for databases)
- SQLite queries can be entered manually or generated by the AI model in the query portal
- Query results can be **exported to json** for further analisys
- All the databases and tables in the current directory, are represented in a **tree-shaped structure** for a more intuitive visualization, obtained exploiting `QTreeWidget`, imported by `PyQt5`

---

### PASSWORD HASHING SYSTEM
MonkeyDB implements a secure password hashing system using the `bcrypt` library.
This ensures that the passwords are never stored in plain text, instead, they are converted irreversibely into cryptographic hashes (*utf-8*)

##### HOW IT WORKS
*User Registration*
- When a new user signs up, the password they enter is passed to the `insert_user()` function
- The password is then encrypted in UTF-8 and hashed from the following code:

       python

       bcrypt.hashpw(password.encode("utf-8"), bcrypt.gensalt())
       

- `bcrypt.gensalt()` automatically generates a secure, random salt to make each hash unique, even if two users have the same password.
- The resulting hashed password is then stored in the local "Users.db" SQLite database.

*User Login*
- When a user tries to login, the system retrieves the stored hash from the database via the function `get_user()`.
- The plain-text password entered during the login, is encoded and checked against the stored hash through the following code:

        python
    
        bcrypt.checkpw(entered_password.encode("utf-8"), stored_hash)
        
- If the comparison is successful, access is granted, otherwise, the login is denied

*Security Benefits*
- Password hashes cannot be reversed to the original passwords.
- bcrypt unique salts prevents attackers from using precomputed hash tables to crack passwords.
- Even if the file "Users.db" is compromised, the attacker cannot easily reconstruct the actual passwords .

---

**Note:**
This system is purely local, hence inadapt to a professional environment

---

### IMPLEMENTATION OF THE AI AGENT
I am an Artificial Intelligence student, and I strongly wanted to
implement some sort of AI agent in my program, however, since I am at the
first year, I do not have any rigorous knowledge about LLMs and NLP,
however, I tried to study on my own and fine tune a small model for
translating natural English into SQL.

#### MODEL

Since my computer is not powerful enought to run a large model, I decided to use a quite small model.
I started from the pretrained `"t5-small"` (60M parameters), imported from the Hugging Face's library `transformers`.

    python
    
    from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments
    from datasets import Dataset
    import pandas as pd
    import os

    model_name = "t5-small"
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

---

#### DATASETS

At the beginning, my idea was to perform a single training session, and I made ChatGPT generate a csv file with 15000 examples, howevwer, afeter the training, the model performed quite well, but struggled in detecting the database schema from the natural language prompt, even if specified.
At this point I decided to do a second training session focusing mainly on this aspect, and I generated a second csv file (16000 examples) with a wider range of database schemas in the examples.

The datasets were red as follow:

    python

    # Upload csv
    data_file = pd.read_csv("csv_file.csv")

    # Dataset preparation
    dataset = Dataset.from_pandas(data_file)
        

Before the training, data pass through the `preprocessing()` function:

    python

    def preprocessing(example):
    input_enc = tokenizer("Translate from English to SQL: " + example["input_text"], truncation=True, padding="max_length", max_length=128)
    target_enc = tokenizer(example["target_text"], truncation=True, padding="max_length", max_length=128)
    return {
        'input_ids': input_enc['input_ids'],
        'attention_mask': input_enc['attention_mask'],
        'output_ids': target_enc['input_ids'],
    }

 
This function prepares the dataset examples for training the T5 model.
- It takes a single `example` containing two text fields:
    - `example["input_text"]`: the natural language prompt in English
    - `example["target_text"]`: the corresponding SQL query
- It uses the tokenizer to:
    - Extend the prompt  with the following string: `Translate from English to SQL`, with truncation, padding to a fixed length (`max_length=128`), and attention masks for the model.
    Specifically useful for t5-small since the pretraining has been done with input texts always starting like this
    - Encode the SQL query with the same truncation and padding settings
- It returns a dictionary containing:
    - `input_ids`: token IDs for the model input
    - `attention_mask`: a mask to ignore padded tokens during the attention process
    - `output_ids`: token IDs for the model output

Finally the data are processed with

    python
    
    tokenized = dataset.map(preprocessing)

This applies `preprocessing()` to each example in the dataset, producing a tokenized dataset ready for the training process

In the second training session I focused more on making the model understand how to derive the db scherma from the nl prompt.
Both training sessions has been performed using google colab, to exploit a GPU (T4 Tesla).

The datasets were quite big, so I started with a small learning rate, *3e-4* and since the loss were costantly decreasing through both training sessions, I decided to keep it unchanged on both training sessions.

---

#### TRAINING PARAMETERS

The training arguments, have been initialized with `Seq2SeqTrainingArguments()`, and kept equal in both training sessions


    python

    training_args = Seq2SeqTrainingArguments(
        output_dir="./t5_sql_finetuned",
        learning_rate=3e-4,
        num_train_epochs=5,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
    
        )

- `num_train_epochs`: the model has been trained for 5 epochs
- `eval_strategy`,`save_strategy` and `logging_strategy` have been initialized to `"epoch"` so that evaluation, saving of each checkpoint and log registering occurs at the end of each epoch

Then I used `Seq2SeqTrainer`, with the chosen arguments, to train the model.

**Training 1 Output**
 
TrainOutput(global_step=15000, training_loss=0.0003296191771825155,
metrics={'train_runtime': 1631.9929, 'train_samples_per_second': 36.765,
'train_steps_per_second': 9.191, 'total_flos': 2030127022080000.0,
'train_loss': 0.0003296191771825155, 'epoch': 5.0})

---

**Training 2 Output**

TrainOutput(global_step=16000, training_loss=0.0038353410586714743, metrics={'train_runtime': 1705.1938, 'train_samples_per_second': 37.532, 'train_steps_per_second': 9.383, 'total_flos': 2165468823552000.0, 'train_loss': 0.0038353410586714743, 'epoch': 5.0})

---

As I said, even if the model is close to some correct SQL query, due to some computational resources limitations, I used a relatively small model, that even with a perfect fine-tuning would struggle to detect the most complex relations and mechanisms in SQL, however, as you can see from the training outputs, I got a relatively small loss, and can be mapped to different reasons, like a possible overfitting of data, however, the model is very close to an acceptable performance, so maybe the training has just gone as well as I expected.

The actual code of the training is not available in the uploaded directory, since it doesn't need to be ran anymore, in fact, after the training the fine-tuned model has been saved and used in `main.py` to create the function `query_from_nl_to_sql(text)`.


**Note**
This implementation is purely demonstrative and, as the whole program, is not, meant to be a professional tool. The model still makes a lot of mistakes.

---

### HOW TO USE MonkeyDB

#### Requirements

Before starting, make sure to have python installed on your pc.
This project makes use of different python libraries that can be installed using pip.

1. **Install the libraries**

    Open your terminal or command prompt to install the dependencies listed in requirements.txt

        bash
        pip install -r requirements.txt
2. **Run the application**

    Navigate through the project's directory in your terminal and execute the main script

        bash
        python db_manager.py

    The application will run and you will visualize the login window
3. **User authentication**

    - *Sign up*: if you are a new user, click on the "sign up" button. You will be directed to the sign up window where you will be able to insert your data, a username and a password. Click on "sign up" to create an account

    - *Login*: If you have an account, insert your username and password in the Login window, the ststem will verify the correspondence
4. **Workspace**

    Once succesfully logged, you will be directed to the main window

    - *Database explorer*: on the leften side of the window, you will visualize all the `.db` files in the current directory.

    - *Change directory*: to change the directory where you want to work, you can use the "open folder" button

5. **Databases and tables managment**

    - *Create a new database*: click on the button "new database" to create a new `.db` file in the current directory

    - *Delete a database or a table*:
        
        1. From the treeview, select the database/table you want to delete
        2. Click on the "Delete database" or "Delete table" button
        3. A confirm window will appear to ultimate the operation. Pay attention since the operation is not reversible.

6. **Execute a query**

    - *SQL query editor*: the query editor is the large square at the center of the workspace

    - *Execute a query*:

        1. First. select the database you want to query from the explorer, otherwise, the query will not run
        2. Write your query on the SQL editor
        3. Click "Execute query" to run the query
        4. The results will be visualized in the "Results" table on the right of the editor
        5. The query will also be saved in the cronology after the execution

    - *Clearing the editor and the table*: use the "Clear" button to clear the query editor and "Clear results" to clear the results table

7. **AI agent usage**

    To obtain an SQL query  starting from natural language, type your prompt on the query editor, then press "Ask AI" to make the model elaborate your request and generate a query.

8. **Results export to JSON format**

    After the execution of a query and the visualization of its results, click on the "Export to Josn" button, you will be able to save the results of the query as a `.json` file

